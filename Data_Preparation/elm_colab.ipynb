{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ELM Data Preparation Pipeline - Google Colab\n\nThis notebook implements a complete data preparation pipeline for Embedding Language Models (ELM) using:\n- **Dataset**: WikiText-2 from HuggingFace\n- **Model**: Qwen3-Embedding-4B (2560-dimensional embeddings)\n\n## Pipeline Overview\n1. Setup & Installation\n2. Configuration\n3. Download WikiText-2\n4. Extract & Clean Paragraphs\n5. Filter by Token Count\n6. Split Dataset\n7. Save Processed Data\n8. Generate Embeddings\n9. Load & Use Dataset\n\n**Estimated Runtime**: 10-20 minutes (depending on GPU availability)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation\n",
    "\n",
    "Install required packages for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.51.0 datasets>=2.0.0 polars>=0.20.0 safetensors>=0.4.0 tqdm\n",
    "\n",
    "# Install flash-attention (optional, improves performance)\n",
    "# Note: This may take a few minutes to compile\n",
    "!pip install -q flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from safetensors.numpy import save_file, load_file\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: No GPU detected. This will be very slow on CPU.\")\n",
    "    print(\"  Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Mount Google Drive for Persistent Storage\n",
    "\n",
    "Uncomment and run this cell if you want to save data to Google Drive (recommended for large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Use Drive for data storage\n",
    "# BASE_DIR = Path('/content/drive/MyDrive/elm_data')\n",
    "# BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# print(f\"Using Google Drive: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "Define configuration settings for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass Config:\n    \"\"\"Configuration for the ELM data preparation pipeline.\"\"\"\n    \n    # Paths (Colab-friendly)\n    base_dir: Path = field(default_factory=lambda: Path('/content/elm_data'))\n    data_dir: Path = field(init=False)\n    processed_dir: Path = field(init=False)\n    embeddings_dir: Path = field(init=False)\n    \n    # Dataset parameters\n    hf_dataset_name: str = \"Salesforce/wikitext\"\n    hf_dataset_config: str = \"wikitext-2-v1\"\n    \n    # Text filtering parameters\n    min_tokens: int = 100\n    max_tokens: int = 2000\n    \n    # Dataset split ratios\n    train_ratio: float = 0.8\n    val_ratio: float = 0.1\n    test_ratio: float = 0.1\n    random_seed: int = 42\n    \n    # Embedding model parameters\n    model_name: str = \"Qwen/Qwen3-Embedding-4B\"\n    embedding_dim: int = 2560\n    max_length: int = 8192\n    batch_size: int = 4  # Reduced for Colab T4 GPU (16GB VRAM)\n    \n    # Model optimization parameters\n    use_flash_attention: bool = True\n    use_fp16: bool = True\n    \n    def __post_init__(self):\n        \"\"\"Initialize derived paths.\"\"\"\n        self.data_dir = self.base_dir / \"data\"\n        self.processed_dir = self.data_dir / \"wikitext2_processed\"\n        self.embeddings_dir = self.data_dir / \"embeddings\"\n        \n        # Validate ratios\n        if not abs(self.train_ratio + self.val_ratio + self.test_ratio - 1.0) < 1e-6:\n            raise ValueError(\"Train, val, and test ratios must sum to 1.0\")\n    \n    def create_directories(self):\n        \"\"\"Create necessary directories if they don't exist.\"\"\"\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        self.processed_dir.mkdir(parents=True, exist_ok=True)\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n    \n    def get_processed_path(self, split: str) -> Path:\n        \"\"\"Get path to processed data file for a given split.\"\"\"\n        return self.processed_dir / f\"{split}.parquet\"\n    \n    def get_embeddings_path(self, split: str) -> Path:\n        \"\"\"Get path to embeddings file for a given split.\"\"\"\n        return self.embeddings_dir / f\"{split}_embeddings.safetensors\"\n\n# Initialize configuration\nconfig = Config()\nconfig.create_directories()\n\nprint(\"Configuration:\")\nprint(f\"  Base directory: {config.base_dir}\")\nprint(f\"  Model: {config.model_name}\")\nprint(f\"  Token range: {config.min_tokens}-{config.max_tokens}\")\nprint(f\"  Batch size: {config.batch_size}\")\nprint(f\"  Split: {config.train_ratio}/{config.val_ratio}/{config.test_ratio}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utility Functions\n",
    "\n",
    "Helper functions for formatting and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Format seconds as human-readable string.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{minutes}m {secs:.0f}s\"\n",
    "    else:\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{hours}h {minutes}m {secs:.0f}s\"\n",
    "\n",
    "def format_size(num_bytes: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable string.\"\"\"\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if num_bytes < 1024.0:\n",
    "            return f\"{num_bytes:.2f} {unit}\"\n",
    "        num_bytes /= 1024.0\n",
    "    return f\"{num_bytes:.2f} PB\"\n",
    "\n",
    "def count_parameters(model) -> int:\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. STEP 1: Download WikiText-2 Dataset\n\nDownload the WikiText-2 dataset from HuggingFace."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"STEP 1: Loading WikiText-2 dataset\")\nprint(\"=\" * 80)\n\nstart_time = time.time()\n\n# Load dataset from HuggingFace\ndataset = load_dataset(\n    config.hf_dataset_name,\n    config.hf_dataset_config,\n    trust_remote_code=False\n)\n\nprint(f\"\\n✓ Dataset loaded successfully in {format_time(time.time() - start_time)}\")\nprint(f\"  Train samples: {len(dataset['train'])}\")\nprint(f\"  Validation samples: {len(dataset['validation'])}\")\nprint(f\"  Test samples: {len(dataset['test'])}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. STEP 2: Extract & Clean Paragraphs\n",
    "\n",
    "Extract paragraphs from the dataset and clean Wikipedia formatting artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean Wikipedia formatting artifacts from text.\"\"\"\n",
    "    # Remove Wikipedia section headers (e.g., \" = = Section = = \")\n",
    "    text = re.sub(r'\\s*=\\s+=\\s+.*?\\s+=\\s+=\\s*', ' ', text)\n",
    "    text = re.sub(r'\\s*=\\s+.*?\\s+=\\s*', ' ', text)\n",
    "    \n",
    "    # Remove Wikipedia formatting artifacts\n",
    "    text = text.replace(\"@-@\", \"-\")\n",
    "    text = text.replace(\"@.@\", \".\")\n",
    "    text = text.replace(\"@,@\", \",\")\n",
    "    \n",
    "    # Remove <unk> tokens\n",
    "    text = text.replace(\"<unk>\", \"\")\n",
    "    \n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_low_quality(text: str) -> bool:\n",
    "    \"\"\"Check if text is low quality and should be filtered out.\"\"\"\n",
    "    if not text:\n",
    "        return True\n",
    "    \n",
    "    total_chars = len(text)\n",
    "    if total_chars == 0:\n",
    "        return True\n",
    "    \n",
    "    # Count special characters and digits\n",
    "    special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())\n",
    "    digits = sum(1 for c in text if c.isdigit())\n",
    "    \n",
    "    # Filter if mostly special characters (>50%)\n",
    "    if special_chars / total_chars > 0.5:\n",
    "        return True\n",
    "    \n",
    "    # Filter if mostly digits (>50%)\n",
    "    if digits / total_chars > 0.5:\n",
    "        return True\n",
    "    \n",
    "    # Filter very short texts (< 20 characters)\n",
    "    if total_chars < 20:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_paragraphs(dataset: DatasetDict) -> List[str]:\n",
    "    \"\"\"Extract paragraphs from HuggingFace dataset.\"\"\"\n",
    "    all_paragraphs = []\n",
    "    current_paragraph = []\n",
    "    \n",
    "    # Process all splits together\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        print(f\"  Processing {split} split...\")\n",
    "        \n",
    "        for item in tqdm(dataset[split], desc=f\"  Extracting from {split}\"):\n",
    "            text = item.get(\"text\", \"\").strip()\n",
    "            \n",
    "            # Empty line indicates paragraph break\n",
    "            if not text:\n",
    "                if current_paragraph:\n",
    "                    paragraph = \" \".join(current_paragraph)\n",
    "                    all_paragraphs.append(paragraph)\n",
    "                    current_paragraph = []\n",
    "            else:\n",
    "                current_paragraph.append(text)\n",
    "        \n",
    "        # Don't forget the last paragraph\n",
    "        if current_paragraph:\n",
    "            paragraph = \" \".join(current_paragraph)\n",
    "            all_paragraphs.append(paragraph)\n",
    "            current_paragraph = []\n",
    "    \n",
    "    return all_paragraphs\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Extracting and preprocessing paragraphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "paragraphs = extract_paragraphs(dataset)\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(paragraphs):,} raw paragraphs in {format_time(time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. STEP 3: Filter Paragraphs by Token Count\n",
    "\n",
    "Filter paragraphs to keep only those with 100-2000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: Filtering paragraphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load tokenizer for filtering\n",
    "print(f\"Loading tokenizer: {config.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# Filter paragraphs\n",
    "filtered_data = []\n",
    "\n",
    "print(f\"\\nFiltering paragraphs (token range: {config.min_tokens}-{config.max_tokens})...\")\n",
    "for text in tqdm(paragraphs, desc=\"Filtering\"):\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Skip low-quality text\n",
    "    if is_low_quality(cleaned_text):\n",
    "        continue\n",
    "    \n",
    "    # Tokenize to count tokens\n",
    "    tokens = tokenizer.encode(cleaned_text, add_special_tokens=False)\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # Filter by token count\n",
    "    if config.min_tokens <= num_tokens <= config.max_tokens:\n",
    "        filtered_data.append({\n",
    "            \"text\": cleaned_text,\n",
    "            \"token_count\": num_tokens,\n",
    "            \"char_count\": len(cleaned_text),\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Kept {len(filtered_data):,}/{len(paragraphs):,} paragraphs \"\n",
    "      f\"({100 * len(filtered_data) / len(paragraphs):.1f}%)\")\n",
    "print(f\"  Completed in {format_time(time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. STEP 4: Split Dataset\n",
    "\n",
    "Split the filtered data into train/validation/test sets (80/10/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: Splitting dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Shuffle data with fixed seed\n",
    "random.seed(config.random_seed)\n",
    "shuffled_data = filtered_data.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "# Calculate split indices\n",
    "n_total = len(shuffled_data)\n",
    "n_train = int(n_total * config.train_ratio)\n",
    "n_val = int(n_total * config.val_ratio)\n",
    "\n",
    "# Split the data\n",
    "train_data = shuffled_data[:n_train]\n",
    "val_data = shuffled_data[n_train:n_train + n_val]\n",
    "test_data = shuffled_data[n_train + n_val:]\n",
    "\n",
    "print(f\"\\n✓ Dataset split ({config.train_ratio}/{config.val_ratio}/{config.test_ratio}):\")\n",
    "print(f\"  Train: {len(train_data):,} samples\")\n",
    "print(f\"  Val: {len(val_data):,} samples\")\n",
    "print(f\"  Test: {len(test_data):,} samples\")\n",
    "print(f\"  Total: {len(train_data) + len(val_data) + len(test_data):,} samples\")\n",
    "print(f\"  Completed in {format_time(time.time() - start_time)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. STEP 5: Save Processed Data\n",
    "\n",
    "Save the processed text data as Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Saving processed data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add text IDs to each split\n",
    "for idx, item in enumerate(train_data):\n",
    "    item[\"text_id\"] = f\"train_{idx}\"\n",
    "\n",
    "for idx, item in enumerate(val_data):\n",
    "    item[\"text_id\"] = f\"val_{idx}\"\n",
    "\n",
    "for idx, item in enumerate(test_data):\n",
    "    item[\"text_id\"] = f\"test_{idx}\"\n",
    "\n",
    "# Save each split\n",
    "splits = {\n",
    "    \"train\": train_data,\n",
    "    \"val\": val_data,\n",
    "    \"test\": test_data,\n",
    "}\n",
    "\n",
    "for split_name, split_data in splits.items():\n",
    "    output_path = config.get_processed_path(split_name)\n",
    "    \n",
    "    # Convert to Polars DataFrame and save\n",
    "    df = pl.DataFrame(split_data)\n",
    "    df.write_parquet(output_path)\n",
    "    \n",
    "    file_size = output_path.stat().st_size\n",
    "    print(f\"  ✓ Saved {split_name}: {output_path.name} ({len(split_data):,} samples, {format_size(file_size)})\")\n",
    "\n",
    "print(f\"\\n✓ All data saved successfully in {format_time(time.time() - start_time)}\")\n",
    "print(f\"  Output directory: {config.processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. STEP 6: Generate Embeddings\n",
    "\n",
    "Load the Qwen3-Embedding-4B model and generate embeddings for all splits.\n",
    "\n",
    "**Note**: This step takes the longest (1-2 hours depending on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    \"\"\"Extract embeddings from the last token position.\"\"\"\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device),\n",
    "            sequence_lengths\n",
    "        ]\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings_batch(\n",
    "    model: AutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    config: Config,\n",
    "    desc: str = \"Generating embeddings\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(texts) + config.batch_size - 1) // config.batch_size\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), config.batch_size), total=num_batches, desc=desc):\n",
    "        batch_texts = texts[i:i + config.batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_dict = {k: v.to(model.device) for k, v in batch_dict.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch_dict)\n",
    "        \n",
    "        # Extract embeddings using last token pooling\n",
    "        embeddings = last_token_pool(\n",
    "            outputs.last_hidden_state,\n",
    "            batch_dict['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Normalize embeddings (L2 norm)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        embeddings_np = embeddings.cpu().float().numpy()\n",
    "        all_embeddings.append(embeddings_np)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings\n",
    "\n",
    "print(\"✓ Embedding functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: Generating embeddings\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load embedding model\n",
    "print(f\"\\nLoading embedding model: {config.model_name}\")\n",
    "\n",
    "# Load tokenizer with left padding\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "# Load model with optimizations\n",
    "try:\n",
    "    if config.use_flash_attention and torch.cuda.is_available():\n",
    "        print(\"  Loading with flash_attention_2...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            config.model_name,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            torch_dtype=torch.float16 if config.use_fp16 else torch.float32\n",
    "        )\n",
    "    else:\n",
    "        print(\"  Loading without flash attention...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            config.model_name,\n",
    "            torch_dtype=torch.float16 if config.use_fp16 else torch.float32\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"  Warning: {e}\")\n",
    "    print(\"  Falling back to standard attention...\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.float16 if config.use_fp16 else torch.float32\n",
    "    )\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(f\"  ✓ Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"  ⚠ Using CPU (this will be VERY slow)\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f\"  Model parameters: {num_params:,}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Max length: {config.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each split\n",
    "print(\"\\nGenerating embeddings for all splits...\\n\")\n",
    "\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    \n",
    "    # Load texts from parquet\n",
    "    parquet_path = config.get_processed_path(split_name)\n",
    "    df = pl.read_parquet(parquet_path)\n",
    "    texts = df[\"text\"].to_list()\n",
    "    \n",
    "    print(f\"  Loaded {len(texts):,} texts\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings_batch(\n",
    "        model=model,\n",
    "        tokenizer=embedding_tokenizer,\n",
    "        texts=texts,\n",
    "        config=config,\n",
    "        desc=f\"  Embedding {split_name}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  Generated embeddings shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Save embeddings\n",
    "    output_path = config.get_embeddings_path(split_name)\n",
    "    \n",
    "    tensors = {\"embeddings\": embeddings}\n",
    "    metadata = {\n",
    "        \"split\": split_name,\n",
    "        \"model\": config.model_name,\n",
    "        \"num_texts\": str(len(texts)),\n",
    "        \"shape\": str(embeddings.shape),\n",
    "        \"dtype\": str(embeddings.dtype),\n",
    "    }\n",
    "    \n",
    "    save_file(tensors, str(output_path), metadata=metadata)\n",
    "    \n",
    "    file_size = output_path.stat().st_size\n",
    "    print(f\"  ✓ Saved to: {output_path.name} ({format_size(file_size)})\\n\")\n",
    "\n",
    "print(f\"✓ All embeddings completed in {format_time(time.time() - start_time)}\")\n",
    "print(f\"  Output directory: {config.embeddings_dir}\")\n",
    "\n",
    "# Free up GPU memory\n",
    "del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"  GPU memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. STEP 7: Dataset Class\n",
    "\n",
    "Define PyTorch Dataset class for loading and using the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMDataset(Dataset):\n",
    "    \"\"\"Dataset class for ELM that loads text, embeddings, and metadata.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: Path,\n",
    "        split: str = \"train\",\n",
    "        load_embeddings: bool = True,\n",
    "        config: Optional[Config] = None\n",
    "    ):\n",
    "        \"\"\"Initialize ELMDataset.\"\"\"\n",
    "        self.split = split\n",
    "        self.load_embeddings = load_embeddings\n",
    "        \n",
    "        if config is None:\n",
    "            config = Config()\n",
    "            config.base_dir = data_dir.parent if data_dir.name == \"data\" else data_dir\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Load text data\n",
    "        self.parquet_path = config.get_processed_path(split)\n",
    "        if not self.parquet_path.exists():\n",
    "            raise FileNotFoundError(f\"{split} data file not found: {self.parquet_path}\")\n",
    "        \n",
    "        self.df = pl.read_parquet(self.parquet_path)\n",
    "        \n",
    "        # Extract columns\n",
    "        self.texts = self.df[\"text\"].to_list()\n",
    "        self.text_ids = self.df[\"text_id\"].to_list()\n",
    "        self.token_counts = self.df[\"token_count\"].to_list()\n",
    "        self.char_counts = self.df[\"char_count\"].to_list()\n",
    "        \n",
    "        # Load embeddings if requested\n",
    "        self.embeddings = None\n",
    "        if load_embeddings:\n",
    "            self.embeddings_path = config.get_embeddings_path(split)\n",
    "            if not self.embeddings_path.exists():\n",
    "                raise FileNotFoundError(f\"{split} embeddings file not found: {self.embeddings_path}\")\n",
    "            \n",
    "            tensors = load_file(str(self.embeddings_path))\n",
    "            self.embeddings = tensors[\"embeddings\"]\n",
    "            \n",
    "            # Validate dimensions match\n",
    "            if len(self.embeddings) != len(self.texts):\n",
    "                raise ValueError(\n",
    "                    f\"Mismatch between texts ({len(self.texts)}) \"\n",
    "                    f\"and embeddings ({len(self.embeddings)})\"\n",
    "                )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"Get a single sample.\"\"\"\n",
    "        item = {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"metadata\": {\n",
    "                \"text_id\": self.text_ids[idx],\n",
    "                \"token_count\": self.token_counts[idx],\n",
    "                \"char_count\": self.char_counts[idx],\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.embeddings is not None:\n",
    "            item[\"embedding\"] = self.embeddings[idx]\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def interpolate_embeddings(self, idx1: int, idx2: int, alpha: float) -> np.ndarray:\n",
    "        \"\"\"Linear interpolation between two embeddings.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"Embeddings not loaded. Set load_embeddings=True\")\n",
    "        \n",
    "        emb1 = self.embeddings[idx1]\n",
    "        emb2 = self.embeddings[idx2]\n",
    "        \n",
    "        # Linear interpolation\n",
    "        interpolated = (1 - alpha) * emb1 + alpha * emb2\n",
    "        \n",
    "        # Re-normalize to unit length\n",
    "        norm = np.linalg.norm(interpolated)\n",
    "        if norm > 0:\n",
    "            interpolated = interpolated / norm\n",
    "        \n",
    "        return interpolated\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get dataset statistics.\"\"\"\n",
    "        stats = {\n",
    "            \"split\": self.split,\n",
    "            \"num_samples\": len(self),\n",
    "            \"avg_token_count\": np.mean(self.token_counts),\n",
    "            \"avg_char_count\": np.mean(self.char_counts),\n",
    "            \"min_token_count\": np.min(self.token_counts),\n",
    "            \"max_token_count\": np.max(self.token_counts),\n",
    "        }\n",
    "        \n",
    "        if self.embeddings is not None:\n",
    "            stats[\"embedding_dim\"] = self.embeddings.shape[1]\n",
    "            stats[\"embedding_dtype\"] = str(self.embeddings.dtype)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def get_dataloader(self, batch_size: int, shuffle: bool = False) -> DataLoader:\n",
    "        \"\"\"Create a PyTorch DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=ELMCollator(),\n",
    "        )\n",
    "\n",
    "class ELMCollator:\n",
    "    \"\"\"Collate function for batching variable-length texts.\"\"\"\n",
    "    \n",
    "    def __call__(self, batch: List[Dict]) -> Dict:\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        metadata = [item[\"metadata\"] for item in batch]\n",
    "        \n",
    "        collated = {\n",
    "            \"text\": texts,\n",
    "            \"metadata\": metadata,\n",
    "        }\n",
    "        \n",
    "        if \"embedding\" in batch[0]:\n",
    "            embeddings = np.stack([item[\"embedding\"] for item in batch])\n",
    "            collated[\"embedding\"] = embeddings\n",
    "        \n",
    "        return collated\n",
    "\n",
    "print(\"✓ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. STEP 8: Example Usage & Verification\n",
    "\n",
    "Load the processed data and demonstrate usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 8: Example Usage & Verification\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load training dataset\n",
    "print(\"\\n1. Loading dataset with embeddings...\")\n",
    "train_dataset = ELMDataset(\n",
    "    data_dir=config.data_dir,\n",
    "    split=\"train\",\n",
    "    load_embeddings=True,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"  ✓ Loaded {len(train_dataset):,} samples\")\n",
    "\n",
    "# Get a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n  Sample 0:\")\n",
    "print(f\"    Text: {sample['text'][:100]}...\")\n",
    "print(f\"    Embedding shape: {sample['embedding'].shape}\")\n",
    "print(f\"    Token count: {sample['metadata']['token_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"\\n2. Dataset statistics...\")\n",
    "stats = train_dataset.get_statistics()\n",
    "print(\"\\n  Train dataset:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"    {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataLoader\n",
    "print(\"\\n3. Creating PyTorch DataLoader...\")\n",
    "dataloader = train_dataset.get_dataloader(batch_size=8, shuffle=True)\n",
    "print(f\"  ✓ DataLoader created with {len(dataloader)} batches\")\n",
    "\n",
    "# Get first batch\n",
    "first_batch = next(iter(dataloader))\n",
    "print(f\"\\n  First batch:\")\n",
    "print(f\"    Number of texts: {len(first_batch['text'])}\")\n",
    "print(f\"    Embeddings shape: {first_batch['embedding'].shape}\")\n",
    "print(f\"    Sample text: {first_batch['text'][0][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding interpolation\n",
    "print(\"\\n4. Embedding interpolation...\")\n",
    "idx1, idx2 = 0, 100\n",
    "print(f\"\\n  Interpolating between samples {idx1} and {idx2}:\")\n",
    "print(f\"    Text 1: {train_dataset[idx1]['text'][:60]}...\")\n",
    "print(f\"    Text 2: {train_dataset[idx2]['text'][:60]}...\")\n",
    "\n",
    "print(\"\\n  Interpolated embeddings:\")\n",
    "for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    interpolated = train_dataset.interpolate_embeddings(idx1, idx2, alpha)\n",
    "    print(f\"    alpha={alpha:.2f}: shape={interpolated.shape}, \"\n",
    "          f\"norm={np.linalg.norm(interpolated):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding similarity\n",
    "print(\"\\n5. Computing embedding similarity...\")\n",
    "emb1 = train_dataset[0]['embedding']\n",
    "emb2 = train_dataset[1]['embedding']\n",
    "emb3 = train_dataset[100]['embedding']\n",
    "\n",
    "# Compute cosine similarity (embeddings are already normalized)\n",
    "sim_01 = np.dot(emb1, emb2)\n",
    "sim_02 = np.dot(emb1, emb3)\n",
    "sim_12 = np.dot(emb2, emb3)\n",
    "\n",
    "print(\"\\n  Cosine similarities:\")\n",
    "print(f\"    Sample 0 vs 1: {sim_01:.4f}\")\n",
    "print(f\"    Sample 0 vs 100: {sim_02:.4f}\")\n",
    "print(f\"    Sample 1 vs 100: {sim_12:.4f}\")\n",
    "\n",
    "print(f\"\\n  Sample 0: {train_dataset[0]['text'][:80]}...\")\n",
    "print(f\"  Sample 1: {train_dataset[1]['text'][:80]}...\")\n",
    "print(f\"  Sample 100: {train_dataset[100]['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all splits\n",
    "print(\"\\n6. Loading all splits...\")\n",
    "splits = {}\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    splits[split_name] = ELMDataset(\n",
    "        data_dir=config.data_dir,\n",
    "        split=split_name,\n",
    "        load_embeddings=True,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "print(\"\\n  Dataset sizes:\")\n",
    "for split_name, dataset in splits.items():\n",
    "    print(f\"    {split_name}: {len(dataset):,} samples\")\n",
    "\n",
    "total_samples = sum(len(ds) for ds in splits.values())\n",
    "print(f\"    Total: {total_samples:,} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  Processed data: {config.processed_dir}\")\n",
    "print(f\"  Embeddings: {config.embeddings_dir}\")\n",
    "print(f\"\\nYou can now use the ELMDataset class to load and use this data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nThis notebook has successfully:\n1. ✓ Downloaded WikiText-2 dataset\n2. ✓ Extracted and cleaned paragraphs\n3. ✓ Filtered by token count (100-2000 tokens)\n4. ✓ Split into train/val/test (80/10/10)\n5. ✓ Saved processed data as Parquet files\n6. ✓ Generated 2560-dimensional embeddings using Qwen3-Embedding-4B\n7. ✓ Created PyTorch Dataset class for easy loading\n\n### Next Steps\n\nYou can now use the processed data for:\n- Training language models\n- Embedding-based retrieval systems\n- Semantic similarity tasks\n- Text generation experiments\n\n### Downloading Results\n\nTo download the processed data:\n```python\n# Zip the output directory\n!zip -r elm_data.zip /content/elm_data/\n\n# Download (in Colab)\nfrom google.colab import files\nfiles.download('elm_data.zip')\n```\n\nOr use the Google Drive mount option at the beginning to save directly to your Drive."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}